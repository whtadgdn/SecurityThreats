---
title: "Исследование метаданных DNS трафика"
subtitle: "Отчет по практике 4"
author: "Zid4a84@yandex.ru"
format: 
  md:
    output-file: README.md
---

## Цель работы

1.  Зекрепить практические навыки использования языка программирования R для обработки данных
2.  Закрепить знания основных функций обработки данных экосистемы tidyverse языка R
3.  Закрепить навыки исследования метаданных DNS трафика

## Исходные данные

1.  Программное обеспечение ОС Windows 11 Pro
2.  RStudio
3.  Интерпретатор языка R 4.5.1

## План

1. Импортируйте данные DNS – https://storage.yandexcloud.net/dataset.ctfsec/dns.zip
Данные были собраны с помощью сетевого анализатора zeek
2. Добавьте пропущенные данные о структуре данных (назначении столбцов)
3. Преобразуйте данные в столбцах в нужный формат,просмотрите общую структуру данных с помощью функции glimpse()
4. Сколько участников информационного обмена всети Доброй Организации?
5. Какое соотношение участников обмена внутрисети и участников обращений к внешним ресурсам?
6. Найдите топ-10 участников сети, проявляющих наибольшую сетевую активность.
7. Найдите топ-10 доменов, к которым обращаются пользователи сети и соответственное количество обращений
8. Опеределите базовые статистические характеристики (функция summary() ) интервала времени между последовательными обращениями к топ-10 доменам.
9. Часто вредоносное программное обеспечение использует DNS канал в качестве канала управления, периодически отправляя запросы на подконтрольный злоумышленникам DNS сервер. По периодическим запросам на один и тот же домен можно выявить скрытый DNS канал. Есть ли такие IP адреса в исследуемом датасете?
10. Определите местоположение (страну, город) и организацию-провайдера для топ-10 доменов. Для этого можно использовать сторонние сервисы,например http://ip-api.com (API-эндпоинт –
http://ip-api.com/json).

## Шаги:
### 1. Импорт данных
```{r}
# Загрузка и распаковка данных
install.packages("dplyr")
install.packages("tidyverse")
install.packages("jsonlite")
library(httr)
library(jsonlite)
library(dplyr)
library(tidyverse)
download.file("https://storage.yandexcloud.net/dataset.ctfsec/dns.zip", 
              destfile = "dns.zip")
unzip("dns.zip")
dns_data <- read.table("dns.log", header = FALSE, sep = "\t", 
                       comment.char = "#", fill = TRUE)

```

### 2. Добавление названий столбцов
```{r}

colnames(dns_data) <- c("ts", "uid", "id.orig_h", "id.orig_p", 
                        "id.resp_h", "id.resp_p", "proto", "trans_id",
                        "query", "qclass", "qclass_name", "qtype", 
                        "qtype_name", "rcode", "rcode_name", "AA", 
                        "TC", "RD", "RA", "Z", "answers", "TTLs", "rejected")
```

### 3. Преобразование данных в нужный формат
```{r}
dns_data <- dns_data %>%
  mutate(
    ts = as.POSIXct(ts, origin = "1970-01-01"),
    id.orig_p = as.integer(id.orig_p),
    id.resp_p = as.integer(id.resp_p),
    trans_id = as.integer(trans_id)
  )
```

### 4. Просмотр структуры данных
```{r}
glimpse(dns_data)
```

### Анализ данных
```{r}
# Задание 4: Количество участников информационного обмена
unique_participants <- dns_data %>%
  summarise(
    unique_sources = n_distinct(id.orig_h),
    unique_destinations = n_distinct(id.resp_h),
    total_unique = n_distinct(c(id.orig_h, id.resp_h))
  )
print(unique_participants)
```

```{r}
# Задание 5: Соотношение внутренних и внешних участников
is_internal <- function(ip) {
  grepl("^10\\.|^172\\.(1[6-9]|2[0-9]|3[0-1])\\.|^192\\.168\\.", ip)
}

network_ratio <- dns_data %>%
  summarise(
    internal_sources = sum(is_internal(id.orig_h)),
    external_sources = sum(!is_internal(id.orig_h)),
    internal_destinations = sum(is_internal(id.resp_h)),
    external_destinations = sum(!is_internal(id.resp_h))
  )
print(network_ratio)
```

```{r}
# Задание 6: Топ-10 участников с наибольшей активностью
top_10_participants <- dns_data %>%
  count(id.orig_h, name = "requests_count") %>%
  arrange(desc(requests_count)) %>%
  head(10)
print(top_10_participants)
```

```{r}
# Задание 7: Топ-10 доменов по количеству обращений

top_10_domains <- dns_data %>%
  count(query, name = "request_count") %>%
  arrange(desc(request_count)) %>%
  head(10)
print(top_10_domains)

```

```{r}
# Задание 8: Статистика интервалов времени между запросами к топ-10 доменам
top_domains_list <- top_10_domains$query

time_intervals <- dns_data %>%
  filter(query %in% top_domains_list) %>%
  arrange(query, ts) %>%
  group_by(query) %>%
  mutate(time_diff = as.numeric(difftime(ts, lag(ts), units = "secs"))) %>%
  filter(!is.na(time_diff))
print(summary(time_intervals$time_diff))
# Подробная статистика по выборке доменов
time_intervals %>%
  group_by(query) %>%
  summarise(
    mean_interval = mean(time_diff, na.rm = TRUE),
    median_interval = median(time_diff, na.rm = TRUE),
    min_interval = min(time_diff, na.rm = TRUE),
    max_interval = max(time_diff, na.rm = TRUE),
    sd_interval = sd(time_diff, na.rm = TRUE)
  ) %>%
  print()
```

```{r}
# Задание 9: Поиск подозрительных периодических запросов (скрытый DNS канал)
# Ищем IP адреса с регулярными запросами к одному домену
suspicious_activity <- dns_data %>%
  group_by(id.orig_h, query) %>%
  summarise(
    request_count = n(),
    .groups = 'drop'
  ) %>%
  filter(request_count > 10) %>%
  arrange(desc(request_count))
# Анализ периодичности для подозрительных IP
periodic_requests <- dns_data %>%
  semi_join(suspicious_activity, by = c("id.orig_h", "query")) %>%
  arrange(id.orig_h, query, ts) %>%
  group_by(id.orig_h, query) %>%
  mutate(time_diff = as.numeric(difftime(ts, lag(ts), units = "secs"))) %>%
  filter(!is.na(time_diff)) %>%
  summarise(
    request_count = n(),
    mean_interval = mean(time_diff),
    sd_interval = sd(time_diff),
    cv = sd(time_diff) / mean(time_diff),
    .groups = 'drop'
  ) %>%
  filter(cv < 0.5) %>%
  arrange(cv)
print(periodic_requests)
```

```{r}
# Задание 10: Определение местоположения топ-10 доменов
domain_ips <- dns_data %>%
  filter(query %in% top_domains_list) %>%
  select(query, answers) %>%
  filter(!is.na(answers) & answers != "-") %>%
  distinct()

get_geolocation <- function(ip) {
  url <- paste0("http://ip-api.com/json/", ip)
  tryCatch({
    response <- GET(url)
    if (status_code(response) == 200) {
      data <- fromJSON(content(response, "text"))
      return(data.frame(
        ip = ip,
        country = ifelse(is.null(data$country), NA, data$country),
        city = ifelse(is.null(data$city), NA, data$city),
        org = ifelse(is.null(data$org), NA, data$org),
        stringsAsFactors = FALSE
      ))
    }
    return(NULL)
  }, error = function(e) {
    return(NULL)
  })
}
geolocation_results <- data.frame()

for (i in 1:min(10, nrow(domain_ips))) {
  ip <- domain_ips$answers[i]
  domain <- domain_ips$query[i]
  
  geo_data <- get_geolocation(ip)
  
  if (!is.null(geo_data)) {
    geo_data$domain <- domain
    geolocation_results <- rbind(geolocation_results, geo_data)
  }
  
  Sys.sleep(1)
}

print(geolocation_results)
```
## Оценка результата

В рамках практческой работы была исследована подозрительная сетевая активность во внутренней сети Доброй
Организации. Были восстановлены недостающие метаданные и подготовлены ответы на вопросы.

## Вывод

Таким мобразом в ходе работы мы зекрепили практические навыки использования языка программирования R для обработки данных, знания основных функций обработки данных экосистемы tidyverse языка R и навыки исследования метаданных DNS трафика

